{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf5f230",
   "metadata": {},
   "source": [
    "# GradOptics Tutorial\n",
    "\n",
    "This notebook steps through basic examples using [gradoptics](https://github.com/Magis-slac/gradoptics), a tool developed at SLAC. \n",
    "\n",
    "Begin by installing all of the dependencies. \n",
    "\n",
    "**If running in Colab, change your runtime to GPU (Runtime>Change Runtime Type>Hardware Accelerator>T4 GPU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdbbd05",
   "metadata": {},
   "source": [
    "Basic dependencies -- this tutorial isn't too picky about exact versions. **If running in Colab, feel free to skip the install of these dependencies, the base environment should work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9589b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --find-links https://download.pytorch.org/whl/torch_stable.html \"torch==1.13.1+cu117\" numpy==1.23.1 matplotlib==3.5.2 scipy==1.8.1 tqdm==4.64.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbcb797",
   "metadata": {},
   "source": [
    "gradoptics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradoptics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b955bfc4",
   "metadata": {},
   "source": [
    "If running in Colab, need some files in the path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sgasioro/gradoptics-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09639e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'gradoptics-tutorial/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddbf315",
   "metadata": {},
   "source": [
    "gradoptics is a ray-tracing simulator written in PyTorch. It uses the straight-line nature of light, as well as known interactions with optical elements, to render a scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45654aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradoptics as optics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from scipy.spatial.transform import Rotation as R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f366b87",
   "metadata": {},
   "source": [
    "## Setting up a cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d451d6",
   "metadata": {},
   "source": [
    "Suppose we want to image an atom cloud. We include a cloud with a Gaussian envelope and imprinted sinusoidal fringes as a template that might mimic a cloud of interest in MAGIS.\n",
    "\n",
    "`w0` controls the width of the Gaussian envelope (std. dev of the Gaussian). Let's consider a cloud at the origin with a width ~1mm (`w0 = 0.0005`m).\n",
    "\n",
    "**Note: The base unit of length is always meters for gradoptics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_cloud_distribution = optics.AtomCloud(position=[0., 0., 0.], w0=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849a7e6",
   "metadata": {},
   "source": [
    "Let's just see what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = torch.cartesian_prod(torch.linspace(-0.002, 0.002, 100), \n",
    "                           torch.linspace(-0.002, 0.002, 100),\n",
    "                           torch.linspace(-0.002, 0.002, 100))\n",
    "\n",
    "pdf_vals = atom_cloud_distribution.pdf(grid).reshape(100, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441284a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10, 3))\n",
    "ax[0].imshow(pdf_vals.sum(dim=0).T, origin='lower')\n",
    "ax[0].set_title('Sum x')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(pdf_vals.sum(dim=1).T, origin='lower')\n",
    "ax[1].set_title('Sum y')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(pdf_vals.sum(dim=2).T, origin='lower')\n",
    "ax[2].set_title('Sum z')\n",
    "ax[2].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e2520",
   "metadata": {},
   "source": [
    "The object to be imaged in gradoptics is called a light source. In our case, we can define a light source from this atom cloud distribution very simply by wrapping it in the `LightSourceFromDistribution` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cafc5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "light_source = optics.LightSourceFromDistribution(atom_cloud_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc1347",
   "metadata": {},
   "source": [
    "while the atom cloud distribution defines all of the properties associated with the density itself, `LightSourceFromDistribution` uses these to interface with `Rays` -- the fundamental object for the rendering procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632261e",
   "metadata": {},
   "source": [
    "## Setting up a camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6496dec",
   "metadata": {},
   "source": [
    "In order to have an image, we need to have something to capture the image. In `gradoptics`, a camera has two fundamental components: a `Lens` and a `Sensor`. We have implemented both thin and thick lenses -- for simplicity, we'll assume a thin lens here (`PerfectLens`).\n",
    "\n",
    "Note that lenses can be composed for more complex optics! They're just objects with lens properties, sitting in space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08170c1e",
   "metadata": {},
   "source": [
    "### PerfectLens\n",
    "\n",
    "gradoptics has a default camera orientation along `+x`. Since our cloud is at `(0, 0, 0)`, we'll put the camera at some fixed position in `-x`. We want our image to be in focus, so let's use the lensmaker's formula, assuming some values for position and magnification, to calculate an appropriate lens focal length. We can, of course, fix any two of these and calculate the third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_focal_length(m, obj_distance):\n",
    "    f =  obj_distance / ((1 / m) + 1)\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df43898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_pos = torch.tensor([-0.06, 0, 0])\n",
    "obj_distance = torch.norm(camera_pos-light_source.distribution.position)\n",
    "m = 0.1\n",
    "f= calculate_focal_length(m, obj_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa02fd70",
   "metadata": {},
   "source": [
    "We also need to define the numerical aperture of the camera, which effects the amount of light we will collect. Usually this is expressed via the lens f-number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical aperture (size of camera opening, f-number)\n",
    "na = 1/1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f6808",
   "metadata": {},
   "source": [
    "With all of these characteristics, we can then define a lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4010d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = optics.PerfectLens(f=f, na=na, position=list(camera_pos), m=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a640347b",
   "metadata": {},
   "source": [
    "### Sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afdeb22",
   "metadata": {},
   "source": [
    "The sensor collects the light to make the final image. For an in focus image, we can calculate the position of the sensor relative to the lens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ab134",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_position = torch.tensor([-f * (1 + m), 0, 0])      \n",
    "sensor_position = camera_pos + rel_position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bea525e",
   "metadata": {},
   "source": [
    "We then need to define properties like the camera **resolution** and **pixel size** as well as other characteristics like quantum efficiency and poisson noise. Viewing direction controls the sensor orientation, and should be pointing towards the lens. We make some choices here for you. Reminder that pixel size, e.g. is in meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319a07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = optics.Sensor(position=sensor_position, viewing_direction=(1,0,0),\n",
    "                       resolution=(200,200), pixel_size=(2.4e-06, 2.4e-06),\n",
    "                       poisson_noise_mean=2, quantum_efficiency=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd84bfa",
   "metadata": {},
   "source": [
    "### Cameras\n",
    "For convenience, the lens and sensor system can be packaged up in a `Camera` object, which also ensures that any light reaching the sensor passes through the lens. For demonstration purposes, this isn't always the most convenient, but as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be70a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = optics.Camera(lens, sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82986d75",
   "metadata": {},
   "source": [
    "## Making a scene!\n",
    "\n",
    "All of the optical components in gradoptics get wrapped up in a `Scene` object, which will get used in the rendering. This is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = optics.Scene(light_source)\n",
    "scene.add_object(sensor)\n",
    "scene.add_object(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fffe050",
   "metadata": {},
   "source": [
    "We can then plot our optical setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781fb7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "scene.plot(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6e922b",
   "metadata": {},
   "source": [
    "# Rendering an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e886d5",
   "metadata": {},
   "source": [
    "Now that we have our optical components and our atom cloud, all that remains is to do the actual image generation. We do so via a procedure called _ray-tracing_. \n",
    "\n",
    "\n",
    "A ray is defined by an origin $\\mathbf{o}$ and a direction $\\mathbf{d}$, with a given point along a ray at time $t$ given by\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{r}(t) = \\mathbf{o} + t\\cdot \\mathbf{d}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "There are two common modes of tracing rays for image generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879faf97",
   "metadata": {},
   "source": [
    "## Forward ray tracing\n",
    "\n",
    "In forward ray tracing, we generate light rays from the light source, and follow them all the way to the camera sensor. This mimics how light is actually produced. However it can be inefficient -- we end up generating a lot of rays that never make it to the sensor. Let's look at this, and learn how to work with rays in the meantime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ca6c9",
   "metadata": {},
   "source": [
    "To generate rays from a light source, we just use the sample rays method (let's keep it small with n_rays=100 for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274952e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rays = scene.light_source.sample_rays(100, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700dce63",
   "metadata": {},
   "source": [
    "Each ray has an origin and a direction, accessed via `rays.origins` and `rays.directions`. Let's plot line segments for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b47cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = 0\n",
    "start = rays.origins\n",
    "\n",
    "# t = 0.01\n",
    "end = rays.origins + 0.01*rays.directions\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "for i in range(len(start)):\n",
    "    ax.plot([start[i, 0], end[i, 0]], \n",
    "            [start[i, 1], end[i, 1]],\n",
    "            [start[i, 2], end[i, 2]], c='C0', alpha=0.4)\n",
    "scene.plot(ax)\n",
    "ax.view_init(0, -90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e2e9c",
   "metadata": {},
   "source": [
    "We can follow these rays through the scene until they hit the sensor. Procedurally, we take each ray, check if it intersects an object in the scene, adjust the ray accordingly, and repeat. This loop is wrapped up in a `forward_ray_tracing` call, and the max number of these steps a ray is allowed ot take is set by `max_iterations`. In this case, we just have cloud to lens, lens to sensor (so `max_iterations = 2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "optics.forward_ray_tracing(rays, scene, max_iterations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf7f63",
   "metadata": {},
   "source": [
    "The sensor object keeps track of which rays intersect. We can then get the resulting image by calling readout. This is where point spread function (PSF) effects are computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = scene.objects[0].readout(add_poisson_noise=False, destructive_readout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b5b655",
   "metadata": {},
   "source": [
    "We've only traced 100 rays, so we don't expect much signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c866f",
   "metadata": {},
   "source": [
    "To generate a meaningful image, we need to trace many more rays. Since we have finite memory, we usually need to do this in batches. We wrap this up in a function for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a11e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image(scene, device='cpu', nb_rays=int(1e9), batch_size=int(50e6), quantum_efficiency=True, max_iterations=2,\n",
    "               add_poisson_noise=True, lookup_table=None, show_progress=True, destructive_readout=True):\n",
    "    \n",
    "    progress_bar = tqdm if show_progress else lambda x: x\n",
    "    nb_rays_left_to_sample = nb_rays\n",
    "    for _ in progress_bar(range(int(np.ceil(nb_rays / batch_size)))):\n",
    "        rays = scene.light_source.sample_rays(min(batch_size, nb_rays_left_to_sample), device=device)\n",
    "        \n",
    "        optics.forward_ray_tracing(rays, scene, max_iterations=max_iterations)\n",
    "        nb_rays_left_to_sample -= batch_size\n",
    "\n",
    "        del rays\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return scene.objects[0].readout(add_poisson_noise=add_poisson_noise, destructive_readout=destructive_readout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422abcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_forward = make_image(scene, nb_rays=int(1e8),device='cuda', add_poisson_noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_forward.cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962192ab",
   "metadata": {},
   "source": [
    "Still a bit noisy -- but we see the structure! To sharpen the image even more, we'll need to trace more rays -- feel free to try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b906b415",
   "metadata": {},
   "source": [
    "### Hardware note\n",
    "We are using the GPU (cuda) here. Using CPU is much much slower (at the time of running here, ~5-10x). Feel free to try running -- but it'll take a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b7fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_forward = make_image(scene, nb_rays=int(1e8), device='cpu', add_poisson_noise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512af4c",
   "metadata": {},
   "source": [
    "## Backward ray tracing\n",
    "\n",
    "In practice it is often more convenient to use backward ray tracing for rendering. This only deals with the light that actually reaches the sensor, allowing us to trace many fewer rays. There are a couple of subtleties here, so we will break it down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a56b32",
   "metadata": {},
   "source": [
    "The key idea of backward ray tracing is that, instead of starting rays at the light source, we start rays at each pixel. The backward part here is that we then trace these rays all the way back to the light source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083fc7af",
   "metadata": {},
   "source": [
    "### Constructing backward rays\n",
    "\n",
    "Let's get the pixel coordinates for the sensor we defined above. For simplicity, we will only consider rays at the center of each pixel. When it comes to dealing with pixel coordinates, there's a useful set of transformations describing how to translate from coordinates on the _2D camera plane_ to coordinates in the _3D world_. What this means for us is we can get pixel centers in a 2D x-y plane.\n",
    "\n",
    "**Big note:** Keeping coordinate systems and sign conventions straight can be a pain (see, e.g. the minus signs in the below cell). Always a good thing to double check -- comparisons with forward ray tracing are useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265cfbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = scene.objects[0]\n",
    "\n",
    "# Pixel coordinates in camera space\n",
    "x = -torch.linspace(-sensor.pixel_size[0]*sensor.resolution[0]/2 + sensor.pixel_size[0]/2,\n",
    "                     sensor.pixel_size[0]*sensor.resolution[0]/2 - sensor.pixel_size[0]/2, \n",
    "                     sensor.resolution[0])\n",
    "\n",
    "y = -torch.linspace(-sensor.pixel_size[1]*sensor.resolution[1]/2 + sensor.pixel_size[1]/2,\n",
    "                     sensor.pixel_size[1]*sensor.resolution[1]/2 - sensor.pixel_size[1]/2, \n",
    "                     sensor.resolution[1])\n",
    "\n",
    "\n",
    "pix_x, pix_y = torch.meshgrid(x, y, indexing='xy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96ccac",
   "metadata": {},
   "source": [
    "Stack this up with an all zeros z coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea2422",
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_z = torch.zeros((sensor.resolution[0], sensor.resolution[1]))\n",
    "\n",
    "all_coords = torch.stack([pix_x, pix_y, pix_z], dim=-1).reshape((-1, 3)).double()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351076a0",
   "metadata": {},
   "source": [
    "And then translate into world space with the appropriate transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df579a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_centers = sensor.c2w.apply_transform_(all_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d9cf8",
   "metadata": {},
   "source": [
    "where `c2w` = camera to world. This is useful if we're moving the camera sensor around -- we just let the c2w transformation handle any world space translation/rotation, and only ever need to worry about the flat pixel grid. Just to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f6f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(pixel_centers[:, 0], pixel_centers[:, 1], pixel_centers[:, 2])\n",
    "scene.plot(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b8e25",
   "metadata": {},
   "source": [
    "In a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ffc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_coords(sensor):\n",
    "    # Pixel coordinates in camera space\n",
    "    x = -torch.linspace(-sensor.pixel_size[0]*sensor.resolution[0]/2 + sensor.pixel_size[0]/2,\n",
    "                        sensor.pixel_size[0]*sensor.resolution[0]/2 - sensor.pixel_size[0]/2, \n",
    "                        sensor.resolution[0])\n",
    "\n",
    "    y = -torch.linspace(-sensor.pixel_size[1]*sensor.resolution[1]/2 + sensor.pixel_size[1]/2,\n",
    "                        sensor.pixel_size[1]*sensor.resolution[1]/2 - sensor.pixel_size[1]/2, \n",
    "                        sensor.resolution[1])\n",
    "    \n",
    "    pix_x, pix_y = torch.meshgrid(x, y, indexing='xy')\n",
    "    \n",
    "    pix_z = torch.zeros((sensor.resolution[0], sensor.resolution[1]))\n",
    "    \n",
    "    all_coords = torch.stack([pix_x, pix_y, pix_z], dim=-1).reshape((-1, 3)).double()\n",
    "    \n",
    "    # Use transforms from above setup to go from pixel space to real (world) space\n",
    "    return sensor.c2w.apply_transform_(all_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279e8cb",
   "metadata": {},
   "source": [
    "These positions are the origins of the rays. To complete the rays, we therefore need a set of directions. \n",
    "\n",
    "\n",
    "We note that the intensity we observe on a given pixel comes from light passing through our lens/camera aperture. If we look at forward ray tracing, light rays can come into the camera from all over the lens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fa7561",
   "metadata": {},
   "outputs": [],
   "source": [
    "rays = scene.light_source.sample_rays(100000, device='cpu')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "optics.forward_ray_tracing(rays, scene, max_iterations=2, ax=ax)\n",
    "scene.plot(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba36f675",
   "metadata": {},
   "source": [
    "In backwards ray tracing, we therefore need to sample the direction of our rays over the lens surface -- we're calculating an integrated effect. For explanatory purposes (and sometimes in general), it is convenient to instead consider a _pinhole_ camera, where we only consider rays that pass through a small point (in our case, the center of the lens).\n",
    "\n",
    "We will come back to simulating a full lens later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc2c049",
   "metadata": {},
   "source": [
    "### Pinhole camera model\n",
    "First we get the center of the lens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c7643",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = scene.objects[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86628b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get directions to center of lens -- this is grabbing the translation component of a 4x4 matrix\n",
    "lens_center = lens.transform.transform[:-1, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13590937",
   "metadata": {},
   "source": [
    "Directions then should be pointing from the sensor to the lens -- can think about it as \"what do i add to origins to get to the lens\":\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{r} = \\mathbf{o} + \\mathbf{d} \\implies \\mathbf{d} = \\mathbf{r} - \\mathbf{o}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdda495",
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = get_pixel_coords(scene.objects[0])\n",
    "\n",
    "directions = lens_center - origins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76ccf3a",
   "metadata": {},
   "source": [
    "And then we can construct rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef852a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rays_sensor_to_lens = optics.Rays(origins, directions, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2646978",
   "metadata": {},
   "source": [
    "Note that directions get normalized to a norm of 1 behind the scenes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3745d89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('original', directions[0])\n",
    "print('normalized', directions[0]/torch.norm(directions[0]))\n",
    "print('ray direction', rays_sensor_to_lens.directions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6d1eaa",
   "metadata": {},
   "source": [
    "Let's check that this is doing what we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = 0\n",
    "start = rays_sensor_to_lens.origins.cpu()\n",
    "\n",
    "# t = 0.01\n",
    "end = (rays_sensor_to_lens.origins + 0.01*rays_sensor_to_lens.directions).cpu()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# Only plot a subset -- we have 200*200 = 40000 rays\n",
    "idxs = np.random.permutation(range(len(start)))[:100]\n",
    "for i in idxs:\n",
    "    ax.plot([start[i, 0], end[i, 0]], \n",
    "            [start[i, 1], end[i, 1]],\n",
    "            [start[i, 2], end[i, 2]], c='C0', alpha=0.4)\n",
    "sensor.plot(ax)\n",
    "lens.plot(ax)\n",
    "ax.view_init(0, -90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331141bb",
   "metadata": {},
   "source": [
    "### From rays to image\n",
    "\n",
    "The contribution of each ray to an observed pixel intensity is via a line integral. This line integral is done via a Monte Carlo integration, where we sample points along the ray and sum them up. The most basic form of this integration would be a pure uniform sample along a given ray. A _stratified sampling_ approach is more common, where sampling is done in uniform bins along the ray. In _hierarchical sampling_, what we do here, additional samples are taken following calculation of importance weights to concentrate samples where the integral is non-zero.\n",
    "\n",
    "\n",
    "So, what do we integrate? In our case, we have a transparent atom cloud, so the contribution of a ray to a given pixel intensity is proportional to the cloud density:\n",
    "\\begin{equation}\n",
    "C(\\mathbf{r}) \\propto \\int \\sigma(\\mathbf{r}(t))dt\n",
    "\\end{equation}\n",
    "\n",
    "In other contexts, additional terms (such as a transmittance), may need to be included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161be668",
   "metadata": {},
   "source": [
    "Most of this is buried behind the scenes, but two important pieces are needed on the user end: an `integrator`, defining the Monte Carlo line integral strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an integrator\n",
    "from gradoptics.integrator import HierarchicalSamplingIntegrator\n",
    "\n",
    "# 64 stratified points, 64 importance weighted points -- may need to adjust this number!\n",
    "integrator = HierarchicalSamplingIntegrator(64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea2c3b",
   "metadata": {},
   "source": [
    "And a `bounding shape`, defining the integration region. This is usually a sphere surrounding the cloud of interest, and helps us be more efficient by focusing Monte Carlo samples around where we actually have density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.light_source.bounding_shape = optics.BoundingSphere(radii=0.01, \n",
    "                                                           xc=0, yc=0, zc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f4162",
   "metadata": {},
   "source": [
    "The ray tracing and integration are then just done via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1742f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_pinhole = optics.backward_ray_tracing(rays_sensor_to_lens, scene, \n",
    "                                          scene.light_source, integrator, max_iterations=2)\n",
    "\n",
    "# Output is flat -- need to reshape into image shape\n",
    "image_pinhole = image_pinhole.reshape(scene.objects[0].resolution).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ecaa2",
   "metadata": {},
   "source": [
    "Resulting in an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_pinhole)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52865c18",
   "metadata": {},
   "source": [
    "which just looks like a projection of our density. This is almost correct, but not quite -- there are a few factors needed for various effects in the integration. We'll just expose them here. Using these factors for the pinhole case is sweeping a couple of things under the rug -- but useful to see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fea555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cos_theta factor -- lens intensity falloff \n",
    "sensor_normal = (lens_center- sensor.position)\n",
    "sensor_normal = sensor_normal*1./torch.norm(sensor_normal)\n",
    "cos_theta = optics.optics.vector.cos_theta(sensor_normal[None, ...], directions.reshape(-1,3)).cuda()\n",
    "\n",
    "\n",
    "# Distance between sensor and lens\n",
    "z = torch.norm((lens_center - sensor.position))\n",
    "\n",
    "# Sensor and lens area\n",
    "pa = 1 / (sensor.resolution[0] * sensor.pixel_size[0] * \n",
    "          sensor.resolution[1] * sensor.pixel_size[1])\n",
    "lens_radius = lens.f * lens.na / 2\n",
    "factor = 1/z**2 * (np.pi * lens_radius**2) * 1/pa\n",
    "\n",
    "\n",
    "image_pinhole = optics.backward_ray_tracing(rays_sensor_to_lens, \n",
    "                                          scene, scene.light_source, \n",
    "                                          integrator, max_iterations=2)\n",
    "\n",
    "image_pinhole = factor*(image_pinhole*cos_theta**4).reshape(sensor.resolution).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa75c36",
   "metadata": {},
   "source": [
    "To properly compare forward and backward, we also need to account for the number of emitted/collected rays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((image_forward.cpu().numpy()/(1e8/(4*np.pi)))/(image_pinhole.cpu()/(200*200)), vmin=0, vmax=2)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee4bfd",
   "metadata": {},
   "source": [
    "Pretty close! Let's wrap the pinhole camera image in a function for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19adf90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_pinhole(scene):\n",
    "    sensor = scene.objects[0]\n",
    "    lens = scene.objects[1]\n",
    "\n",
    "    #Get directions to center of lens -- this is grabbing the translation component of a 4x4 matrix\n",
    "    lens_center = lens.transform.transform[:-1, -1]\n",
    "\n",
    "    origins = get_pixel_coords(sensor)\n",
    "\n",
    "    directions = lens_center - origins\n",
    "\n",
    "\n",
    "    rays_sensor_to_lens = optics.Rays(origins, directions, device='cuda')\n",
    "\n",
    "    # cos_theta factor -- lens intensity falloff \n",
    "    sensor_normal = (lens_center- sensor.position)\n",
    "    sensor_normal = sensor_normal*1./torch.norm(sensor_normal)\n",
    "    cos_theta = optics.optics.vector.cos_theta(sensor_normal[None, ...], directions.reshape(-1,3)).cuda()\n",
    "\n",
    "\n",
    "    # Distance between sensor and lens\n",
    "    z = torch.norm((lens_center - sensor.position))\n",
    "\n",
    "    # Sensor and lens area\n",
    "    pa = 1 / (sensor.resolution[0] * sensor.pixel_size[0] * \n",
    "              sensor.resolution[1] * sensor.pixel_size[1])\n",
    "    lens_radius = lens.f * lens.na / 2\n",
    "    factor = 1/z**2 * (np.pi * lens_radius**2) * 1/pa\n",
    "\n",
    "\n",
    "    image_pinhole = optics.backward_ray_tracing(rays_sensor_to_lens, \n",
    "                                              scene, scene.light_source, \n",
    "                                              integrator, max_iterations=2)\n",
    "\n",
    "    image_pinhole = factor*(image_pinhole*cos_theta**4).reshape(sensor.resolution).cpu()\n",
    "    \n",
    "    return image_pinhole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50036b",
   "metadata": {},
   "source": [
    "## Lens models and backward ray tracing\n",
    "\n",
    "The procedure to include a lens model is to include rays that pass through more parts of the lens than just the center.\n",
    "\n",
    "Why do we need a lens model? This accounts for realistic camera effects, such as _geometric blur_.\n",
    "\n",
    "There are a variety of ways implement this. We'll quickly show one option, contained in the `render_pixels` function. This does a uniform sampling over the lens disk to set ray directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_im_backward(scene):\n",
    "    sensor = scene.objects[0]\n",
    "    lens = scene.objects[1]\n",
    "    \n",
    "    # Pixel indices\n",
    "    idxs = torch.cartesian_prod(torch.arange(sensor.resolution[0]//2, -sensor.resolution[0]//2, -1), \n",
    "                                torch.arange(sensor.resolution[1]//2, -sensor.resolution[1]//2, -1))\n",
    "\n",
    "    pix_x, pix_y = idxs.T\n",
    "\n",
    "\n",
    "    # Batch rays for computation\n",
    "    batch_size = 4000\n",
    "\n",
    "    pix_x = pix_x.split(batch_size)\n",
    "    pix_y = pix_y.split(batch_size)\n",
    "\n",
    "    all_intensities = []\n",
    "    for batch_pix_x, batch_pix_y in tqdm(list(zip(pix_x, pix_y))):\n",
    "        intensities = optics.ray_tracing.ray_tracing.render_pixels(sensor, \n",
    "                                                                   lens,\n",
    "                                                                   scene, scene.light_source, \n",
    "                                                                   10, 10, batch_pix_x,\n",
    "                                                                   batch_pix_y, \n",
    "                                                                   integrator, device='cuda', max_iterations=2)\n",
    "\n",
    "\n",
    "        all_intensities.append(intensities.clone())\n",
    "\n",
    "    # Collect all the results\n",
    "    im_back_lens = torch.cat(all_intensities).reshape(scene.objects[0].resolution).T\n",
    "    \n",
    "    return im_back_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab446fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_back_lens = create_im_backward(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada8c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im_back_lens.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a66a919",
   "metadata": {},
   "source": [
    "Not a lot of difference -- but let's move a little out of focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.light_source.distribution.position = torch.tensor([0.01, 0., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e96fff",
   "metadata": {},
   "source": [
    "The pinhole image doesn't include the lens model, so won't experience the same defocus effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf42b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_pin = create_image_pinhole(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fa655",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im_pin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54b08f",
   "metadata": {},
   "source": [
    "However our lens model will:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86e9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_back_lens_blur = create_im_backward(scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im_back_lens_blur.cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f15ede",
   "metadata": {},
   "source": [
    "We can tune the number of rays in the lens integration to smooth this out. Just to check against forward ray tracing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_forward_blur = make_image(scene, nb_rays=int(1e8),device='cuda', add_poisson_noise=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b4536",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_forward_blur.cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8933b91",
   "metadata": {},
   "source": [
    "Similar defocus! Though will need more rays to sharpen the image (see why we like backward ray tracing?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
